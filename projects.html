<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Projects â€“ Damini Kusum</title>
  <link rel="stylesheet" href="style.css?v=11">
</head>

<body>

  <nav class="nav">
  <div class="nav-inner">
    <a href="index.html" class="nav-link active">About</a>
    <a href="projects.html" class="nav-link">Projects</a>
  </div>
</nav>


  <main style="padding-top: 20vh;">
    <section class="content">
      <h1 style="text-align:center;">Projects</h1>

      <p>
        <strong>Solomonoff Induction and Algorithmic Randomness</strong><br>
        We show that the minimal strength of randomness required for off-sequence convergence is at most 2-computable randomness; this establishes an upper bound on the weakest notion of randomness that ensures the success of Solomonoff induction. Furthermore, due to its rejection of Turing complete reals as random, difference randomness prima facie appears to be the weakest notion that could guarantee convergence. Contrary to this intuition, we prove a negative result with respect to the uniform measure. The machinery of this proof can be extended to show that any notion of randomness that does not guarantee convergence of all c.e.\ supermartingales will always yield a negative result for Solomonoff convergence.
      </p>

    </section>
  </main>

</body>
</html>
